---
title: "432 Class 20"
author: Thomas E. Love, Ph.D.
date: "2024-03-28"
format:
  revealjs: 
    theme: dark
    embed-resources: true
    self-contained: true
    slide-number: true
    footnotes-hover: true
    preview-links: auto
    date-format: iso
    logo: 432-2024-pic.png
    footer: "432 Class 20 | 2024-03-28 | <https://thomaselove.github.io/432-2024/>"
---

## Setup

```{r}
#| echo: true

knitr::opts_chunk$set(comment=NA)
options(width = 80)

library(janitor)
library(gt)
library(lme4)
library(arm)
library(broom)
library(broom.mixed)
library(conflicted)
library(tidyverse)

conflicts_prefer(dplyr::select)

theme_set(theme_bw())
```

## An Introduction to Working with Hierarchical Data

- In a moment, we'll visit <http://mfviz.com/hierarchical-models/>.

There, we try to learn about nested (hierarchical) data on faculty salaries. For each subject (faculty member) in the data, we have information on their salary, department and years of experience.

## Faculty Salaries example

- outcome: faculty salary (in $)
- predictor: years of experience
- group: department (five levels: Informatics, English, Sociology, Biology, Statistics)

We expect that salary (and the relationship between salary and years of experience) may be different depending on  department, and every subject is in exactly one department.

## Visual Explanation

We'll visit <http://mfviz.com/hierarchical-models/> now to learn a bit about:

- Nested Data
- Linear Model on the Fixed Effects
- Adding Random Intercepts to the Fixed Effects Model
- Incorporating Random Slopes with a Constant Intercept
- Random Slope and Random Intercept

## Fitting Hierarchical Models in R

We'll focus today on approaches using the `lme4` package, which can be used both for linear mixed models and for generalized linear mixed models.

- There are many, many ways to do this. 
- The Generalized Linear Mixed Models FAQ at <https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html> describes lots of other options for fitting hierarchical models in R.

## How The Data Were Simulated 

- From Github

```{r}
#| echo: true

# Parameters for generating faculty salary data
departments <- c('sociology', 'biology', 'english', 
                 'informatics', 'statistics')
base.salaries <- c(40000, 50000, 60000, 70000, 80000)
annual.raises <- c(2000, 500, 500, 1700, 500)
faculty.per.dept <- 25
total.faculty <- faculty.per.dept * length(departments)
```

---

```{r}
#| echo: true

# Generate tibble of faculty and (random) years of experience
set.seed(432)
ids <- 1:total.faculty
department <- rep(departments, faculty.per.dept)
experience <- floor(runif(total.faculty, 0, 10))
bases <- rep(base.salaries, faculty.per.dept) * 
    runif(total.faculty, .9, 1.1) # noise
raises <- rep(annual.raises, faculty.per.dept) * 
    runif(total.faculty, .9, 1.1) # noise
facsal <- tibble(ids, department, bases, experience, raises)
# Generate salaries (base + experience * raise)
facsal <- facsal |>
    mutate(salary = bases + experience * raises,
           department = factor(department))
```

## The `facsal` data

```{r}
#| echo: true

facsal
```

## Linear Model (no grouping by department)

```{r}
#| echo: true

m0 <- lm(salary ~ experience, data = facsal)

tidy(m0, conf.int = TRUE) |> 
  select(term, estimate, std.error, 
         conf.low, conf.high) |>
  gt() |> fmt_number(decimals = 2) |> 
  tab_options(table.font.size = 20)
```

## Linear Model Summary

```{r}
#| echo: true

glance(m0) |>
    select(r.squared, adj.r.squared, sigma, AIC, BIC) |>
    gt() |> fmt_number(decimals = 3) |> 
  tab_options(table.font.size = 20)
```

```{r}
#| echo: true

facsal$simple_model_preds <- predict(m0)

head(predict(m0))
```

## Plotting the `m0` predictions and the data

```{r}
#| echo: true
#| output-location: slide

ggplot(data=facsal, aes(x=experience, 
                        y=simple_model_preds)) +
    geom_line(col = "red") + 
    geom_point(aes(x=experience, y=salary)) +
    labs(x="Experience", y="Salary (in $)",
         title = "Linear Model Ignoring Department")
```

## `m0` predictions with Department indicators

```{r}
#| echo: true
#| output-location: slide

ggplot(data=facsal, aes(x=experience, 
                        y=simple_model_preds)) +
  geom_line() + 
  geom_point(aes(x=experience, y=salary, 
                 group = department, colour = department)) +
  labs(x="Experience",y="Salary (in $)",
       title = "Linear Model Ignoring Department") +
  scale_color_discrete('Department') 
```

## `m0` predictions and faceted results by Department

```{r}
#| echo: true
#| output-location: slide

ggplot(data=facsal, aes(x=experience, 
                        y=simple_model_preds)) +
    geom_line() + 
    geom_point(aes(x=experience, y=salary, 
                   group = department, colour = department)) +
    labs(x="Experience",y="Salary (in $)",
         title = "Linear Model Ignoring Department") +
    guides(color = "none") +
    scale_color_discrete('Department') +
    facet_wrap(~ department)
```

## Plot of `m0` Residuals by Department

```{r}
#| echo: true
#| output-location: slide


facsal <- facsal |>
    mutate(simple_model_resids = salary - simple_model_preds)

ggplot(data=facsal, aes(x=department, 
                        y=simple_model_resids)) +
    geom_boxplot() +
    geom_hline(yintercept = 0, col = "red") +
    labs(x="Experience", y="Residuals from Model m0",
       title = "Residuals from Linear Model, by Department")
```

# Let the intercepts vary

## Model incorporating varying intercepts by department

```{r}
#| echo: true

m1 <- lmer(salary ~ experience + (1 | department), 
           data = facsal)

m1
```

## Tidied Coefficients

This is the *varying intercept* model.

```{r}
#| echo: true

tidy(m1, conf.int = TRUE) |>
  select(-std.error, -statistic) |>
  gt() |> fmt_number(decimals = 1) |>
  tab_options(table.font.size = 20)
```

## Summarizing model `m1`

```{r}
#| echo: true

glance(m1) |>
  select(sigma, AIC, BIC, logLik, df.residual) |>
  gt() |> fmt_number(decimals = 2) |>
  tab_options(table.font.size = 20)
```

## Saving the Model `m1` predictions

```{r}
#| echo: true

facsal$random_intercept_preds <- predict(m1)

head(predict(m1))
```

## Plotting the `m1` predictions without the data

```{r}
#| echo: true
#| output-location: slide
ggplot(data=facsal, aes(x=experience, 
                        y=random_intercept_preds, 
                        group = department, 
                        col = department)) +
  geom_line() + 
  labs(x="Experience",y="Salary (in $)",
       title = "Varying Intercept Salary Prediction") +
  scale_color_discrete('Department') 
```

## Plotting the `m1` predictions and the data

```{r}
#| echo: true
#| output-location: slide
ggplot(data=facsal, aes(x=experience, 
                        y=random_intercept_preds, 
                        group = department, 
                        col = department)) +
  geom_line() + 
  geom_point(aes(x=experience, y=salary, 
                 group = department, colour = department)) +
  labs(x="Experience",y="Salary (in $)",
       title = "Varying Intercept Salary Prediction") +
  scale_color_discrete('Department') 
```


## `m1` predictions and the data, faceted by Department

```{r}
#| echo: true
#| output-location: slide
ggplot(data=facsal, aes(x=experience, 
                        y=random_intercept_preds, 
                        group = department, 
                        col = department)) +
  geom_line() + 
  geom_point(aes(x=experience, y=salary, 
                 group = department, colour = department)) +
  labs(x="Experience",y="Salary (in $)",
       title = "Varying Intercept Salary Prediction") +
  scale_color_discrete('Department') +
    facet_wrap(~ department)
```

## Plot of `m1` Residuals by Department

```{r}
#| echo: true
#| output-location: slide
facsal <- facsal |>
    mutate(random_intercept_resids = 
               salary - random_intercept_preds)

ggplot(data=facsal, aes(x=department, 
                        y=random_intercept_resids)) +
    geom_boxplot() +
    geom_hline(yintercept = 0, col = "red") +
    labs(x="Experience", y="Residuals from Model m1",
       title = "Residuals of Varying Intercepts Model")
```

# Let the slopes vary

## Model incorporating varying slopes by department

```{r}
#| echo: true
m2 <- lmer(salary ~ experience + 
               (0 + experience | department), 
           data = facsal)
```

## Varying Slopes Model

```{r}
#| echo: true
m2
```

## Tidied `m2` Coefficients 

```{r}
#| echo: true
tidy(m2, conf.int = TRUE) |>
    select(-std.error, -statistic) |>
    gt() |> fmt_number(decimals = 1) |> 
  tab_options(table.font.size = 20)
```

## Summarizing model `m2`

```{r}
#| echo: true
glance(m2) |>
    select(sigma, AIC, BIC, logLik, df.residual) |>
    gt() |> fmt_number(decimals = 2) |> 
  tab_options(table.font.size = 20)
```

## Saving the Model `m2` predictions

```{r}
#| echo: true
facsal$random_slope_preds <- predict(m2)

head(predict(m2))
```

## Plotting the `m2` predictions without the data

```{r}
#| echo: true
#| output-location: slide
ggplot(data=facsal, aes(x=experience, 
                        y=random_slope_preds, 
                        group = department, 
                        col = department)) +
  geom_line() + 
  labs(x="Experience",y="Salary (in $)",
       title = "Varying Slope Salary Prediction") +
  scale_color_discrete('Department') 
```

## Plotting the `m2` predictions and the data

```{r}
#| echo: true
#| output-location: slide
ggplot(data=facsal, aes(x=experience, 
                        y=random_slope_preds, 
                        group = department, 
                        col = department)) +
  geom_line() + 
  geom_point(aes(x=experience, y=salary, 
                 group = department, colour = department)) +
  labs(x="Experience",y="Salary (in $)",
       title = "Varying Slope Salary Prediction") +
  scale_color_discrete('Department') 
```


## `m2` predictions and the data, faceted by Department

```{r}
#| echo: true
#| output-location: slide
ggplot(data=facsal, aes(x=experience, 
                        y=random_slope_preds, 
                        group = department, 
                        col = department)) +
  geom_line() + 
  geom_point(aes(x=experience, y=salary, 
                 group = department, colour = department)) +
  labs(x="Experience",y="Salary (in $)",
       title = "Varying Slope Salary Prediction") +
  scale_color_discrete('Department') +
    facet_wrap(~ department)
```

## Plot of `m2` Residuals by Department

```{r}
#| echo: true
#| output-location: slide
facsal <- facsal |>
    mutate(random_slope_resids = 
               salary - random_slope_preds)

ggplot(data=facsal, aes(x=department, 
                        y=random_slope_resids)) +
    geom_boxplot() +
    geom_hline(yintercept = 0, col = "red") +
    labs(x="Experience", y="Residuals from Model m2",
       title = "Residuals of Varying Slopes Model")
```

# Let the slopes and intercepts vary

## Model with varying slopes and intercept by department

```{r}
#| echo: true
m3 <- lmer(salary ~ experience + 
               (1 + experience | department), 
           data = facsal)
```

## Varying Slopes and Intercepts Model

```{r}
#| echo: true
m3
```

## Tidied `m3` Coefficients

```{r}
#| echo: true
tidy(m3) |>
    gt() |> fmt_number(decimals = 1) |> 
  tab_options(table.font.size = 20)
```

## Summarizing model `m3`

```{r}
#| echo: true
glance(m3) |>
    select(sigma, AIC, BIC, logLik, df.residual) |>
    gt() |> fmt_number(decimals = 2) |> 
  tab_options(table.font.size = 20)
```

## Saving the Model `m3` predictions

```{r}
#| echo: true
facsal$random_slope_int_preds <- predict(m3)

head(predict(m3))
```

## Plotting the `m3` predictions without the data

```{r}
#| echo: true
#| output-location: slide
ggplot(data=facsal, aes(x=experience, 
                        y=random_slope_int_preds, 
                        group = department, 
                        col = department)) +
  geom_line() + 
  labs(x="Experience",y="Salary (in $)",
       title = "Model m3 Salary Prediction") +
  scale_color_discrete('Department') 
```

## Plotting the `m3` predictions and the data

```{r}
#| echo: true
#| output-location: slide
ggplot(data=facsal, aes(x=experience, 
                        y=random_slope_int_preds, 
                        group = department, 
                        col = department)) +
  geom_line() + 
  geom_point(aes(x=experience, y=salary, 
                 group = department, colour = department)) +
  labs(x="Experience",y="Salary (in $)",
       title = "Model m3 Salary Prediction") +
  scale_color_discrete('Department') 
```


## `m3` predictions and the data, faceted by Department

```{r}
#| echo: true
#| output-location: slide
ggplot(data=facsal, aes(x=experience, 
                        y=random_slope_int_preds, 
                        group = department, 
                        col = department)) +
  geom_line() + 
  geom_point(aes(x=experience, y=salary, 
                 group = department, colour = department)) +
  labs(x="Experience",y="Salary (in $)",
       title = "Model m3 Salary Prediction") +
  scale_color_discrete('Department') +
    facet_wrap(~ department)
```

## Plot of `m3` Residuals by Department

```{r}
#| echo: true
#| output-location: slide
facsal <- facsal |>
    mutate(random_slope_int_resids = 
               salary - random_slope_int_preds)

ggplot(data=facsal, aes(x=department, 
                        y=random_slope_int_resids)) +
    geom_boxplot() +
    geom_hline(yintercept = 0, col = "red") +
    labs(x="Experience", y="Residuals from Model m3",
       title = "Residuals of Model m3")
```

## Comparing the Models

```{r}
#| echo: true
AIC(m0, m1, m2, m3)
```

```{r}
#| echo: true
BIC(m0, m1, m2, m3)
```

## Can we test for an effect of experience?

Let's refit model m3 and compare it to an appropriate null model (without the `experience` information), using an `anova` driven likelihood ratio test.

```{r}
#| echo: true
m3 <- lmer(salary ~ experience + 
               (1 + experience | department), 
           data = facsal, REML = FALSE)

m_null <- lmer(salary ~ (1 | department),
               data = facsal, REML = FALSE)
```

The `REML = FALSE` lets us get the likelihood ratio test we want.

## Likelihood Ratio Test comparing `m3` to `m_null`

```{r}
#| echo: true
anova(m_null, m3)
```

## Tidied coefficients from `m3`

```{r}
#| echo: true
tidy(m3, conf.int = TRUE) |>
    select(-std.error, -statistic) |>
    gt() |> fmt_number(decimals = 1) |> 
  tab_options(table.font.size = 20)
```

## Parametric Bootstrap test for department effect (Part 1)

```{r}
#| echo: true
nBoot=100 # should probably be 1000 at a minimum
lrStat=rep(NA,nBoot)
# first fit appropriate null and alternate models
ft.null <- lm(salary ~ experience, data = facsal) #null model
ft.alt <- lmer(salary ~ experience + (1 | department),
               data=facsal, REML=F) # alternate model
# calculate observed test statistic (deviance = -2 * loglik)
lrObs <- 2*logLik(ft.alt) - 2*logLik(ft.null) # test stat
```

## Parametric Bootstrap test for department effect (Part 2)

```{r}
#| echo: true
set.seed(432)
for(iBoot in 1:nBoot)
{
  facsal$SalSim=unlist(simulate(ft.null)) #resampled data
  # calculate results for our two models in resampled data
  bNull <- lm(SalSim ~ experience, 
              data=facsal) #null model
  bAlt <- lmer(SalSim ~ experience + (1|department),
               data=facsal, REML=F) # alternate model
  # calculate and store resampled test stat
  lrStat[iBoot] <- 2*logLik(bAlt) - 2*logLik(bNull) 
}
```

## Parametric Bootstrap Test for Department effect (Part 3)

```{r}
#| echo: true
mean(lrStat>lrObs) # P-value for test of department effect
```

### Even this "simple" model might not be simple.

Our parametric bootstrap can hit up on the edge of a problem with the random effects.

`boundary (singular) fit: see ?isSingular`

is a common warning we might see, for instance.

## What is a Mixed Model?

A model for an outcome that incorporates both fixed and random effects.

Or, alternatively,...

> Mixed models are those with a mixture of fixed and random effects. Random effects are categorical factors where the levels have been selected from many possible levels and the investigator would like to make inferences beyond just the levels chosen.

- From <http://environmentalcomputing.net/mixed-models/>

## A Random Effect?

A random factor:

- is categorical
- has a large number of levels
- only a subsample (often a random subsample) of levels is included in your design
- you want to make inference in general, and not only for the levels you observed

## A Random Factor?

Think of a random factor as a group where:

- you want to quantify variation between group levels
- you want to make predictions about unobserved groups
- but you don't want to compare outcome differences between particular group levels

Sources: <https://bbolker.github.io/morelia_2018/notes/glmm.html> and <http://environmentalcomputing.net/mixed-models-1/>

## Why Use a Random Effect?

- You want to combine information across groups
- You have variation in information per group level (number of samples or amount of noisiness)
- You have a categorical predictor that is a nuisance variable (something not of direct interest but that we want to control for)
- You have more than 5-6 groups

Source: Crawley (2002) and Gelman (2005) quoted at <https://bbolker.github.io/morelia_2018/notes/glmm.html>

## What is a Fixed Effect vs. a Random Effect?

The one I most often use is something like:

- Fixed effects are constant across individuals, while random effects vary.

The various definitions in the literature are incompatible with each other^[See, for instance, the GLMM FAQ referenced earlier].

## Problems with our definitions

From Scahabenberger and Pierce (2001), we have this gem:

> One modeler's random effect is another modeler's fixed effect.

A more practical definition might be to ask the question posed by Crawley (2002):

> Are there enough levels of the factor in the data on which to base an estimate of the variance of the population of effects? No, means [you should probably treat the variable as] fixed effects.

## Models We Might Consider

Suppose we have an outcome `y`, predictor `x` and group `group`

- `y ~ x` = linear regression on `x`: not a mixed model
- `y ~ 1 + (1 | group)` = random intercept on group: null model
- `y ~ x + (1 | group)` = fixed slope and random intercept
- `y ~ (0 + x | group)` = random slope of x within group, no variation in intercept
- `y ~ x + (x | group)` = random intercept and random slope

## A "More" Realistic Example

The most common example in modern medicine has measurements nested within people. Repeated measures and longitudinal data provide typical settings for this sort of approach.

Another setting where a hierarchical approach is of interest occurs when you have variables measured at multiple levels, for instance you have information on patients, who are nested within providers, who are nested within hospitals.

## Today was just one example

Nothing of what I've talked about today should be taken as the final word on how to extend these ideas beyond the very simple example I've provided this afternoon.

### What's Next?

Survival (time to event) regression models, specifically Cox proportional hazards models.


